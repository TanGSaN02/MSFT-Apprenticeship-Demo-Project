{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "550cc6a6-6ac0-4777-8cb6-b599e25b30cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.secrets.listScopes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78ce5c7f-671c-4635-85c7-ffdf709be72e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install databricks-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7232302-b7e1-4e01-be32-e7e5550efa0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"aprdemoadls\"\n",
    "scope_name = \"keyvaultscopeap\"\n",
    "client_id = dbutils.secrets.get(scope=scope_name, key=\"clientid\")\n",
    "tenant_id = dbutils.secrets.get(scope=scope_name, key=\"tenantid\")\n",
    "client_secret = dbutils.secrets.get(scope=scope_name, key=\"secret\")\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8adb993b-68c6-4ce7-897e-46f27296a71b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container_name = \"bronze\"\n",
    "file_name = \"StockMovement 2\"\n",
    "storage_account_name = \"aprdemoadls\"\n",
    "file_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{file_name}.csv\"\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7324176-8fce-414d-9010-c8d320eed1de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Remove duplicate rows\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28b6965d-97f1-4fdf-97ea-4b3f06961d21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Handle missing values\n",
    "  \n",
    "df = df.dropna()  # Drop rows with any remaining missing values\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13186255-5b01-493c-ac9c-2a518cf58a31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Standardize data types\n",
    "df = df.withColumn('MovementDate', col('MovementDate').cast('date'))\n",
    "df = df.withColumn('Quantity', col('Quantity').cast('double'))\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07ed8e4-29a6-4b82-8460-570178297bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, date_format\n",
    "\n",
    "# Standardize data types\n",
    "df = df.withColumn('MovementDate', col('MovementDate').cast('date'))\n",
    "df = df.withColumn('Quantity', col('Quantity').cast('double'))\n",
    "\n",
    "# Extract time from MovementDate and create a new column\n",
    "df = df.withColumn('MovementTime', date_format(col('MovementDate'), 'HH:mm:ss'))\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14fc456a-0e41-4ce1-a116-42621dc4d396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Group by MovementType and calculate the average Quantity\n",
    "df_grouped = df.groupBy('MovementType').agg(avg('Quantity').alias('AverageQuantity'))\n",
    "\n",
    "display(df_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33d32aeb-eb39-4e90-9cba-a74709cdea46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # %python\n",
    "# # # Save the cleaned DataFrame to Parquet files\n",
    "# # output_path2 = f\"abfss://silver@aprdemoadls.dfs.core.windows.net/StockMovement.parquet\"\n",
    "# # df.write.mode(\"overwrite\").parquet(output_path2)\n",
    "\n",
    "# # Save the cleaned DataFrame to a single Parquet file\n",
    "# output_path2 = \"abfss://silver@aprdemoadls.dfs.core.windows.net/StockMovement.parquet\"\n",
    "# df.coalesce(1).write.mode(\"overwrite\").parquet(output_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ee8247-55b0-4a46-8f59-a47ceec6e2b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Save the cleaned DataFrame to a single Parquet file\n",
    "output_path2 = \"abfss://silver@aprdemoadls.dfs.core.windows.net/StockMovement.parquet\"\n",
    "temp_output_path = \"abfss://silver@aprdemoadls.dfs.core.windows.net/temp_StockMovement.parquet\"\n",
    "\n",
    "# Write the DataFrame to a temporary path\n",
    "df.coalesce(1).write.mode(\"overwrite\").parquet(temp_output_path)\n",
    "\n",
    "# List the files in the temporary directory\n",
    "files = dbutils.fs.ls(temp_output_path)\n",
    "\n",
    "# Find the part file\n",
    "part_file = [file.path for file in files if file.path.endswith(\".parquet\")][0]\n",
    "\n",
    "# Rename the part file to the desired output path\n",
    "dbutils.fs.mv(part_file, output_path2 + \"/StockMovement.parquet\", True)\n",
    "\n",
    "# Remove the temporary directory\n",
    "dbutils.fs.rm(temp_output_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55235c2-aba6-46fb-9e8c-462593299b0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Read the Parquet file to verify its contents\n",
    "df_parquet = spark.read.parquet(output_path2 + \"/StockMovement.parquet\")\n",
    "display(df_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2d5b54e-aa95-4568-a810-5628f32890b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(output_path2)\n",
    "display(df_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492adbd2-9cca-4ec5-a86a-3ea3c0e9295c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Save the cleaned DataFrame to a single CSV file\n",
    "output_path_csv = \"abfss://silver@aprdemoadls.dfs.core.windows.net/StockMovement.csv\"\n",
    "temp_output_path_csv = \"abfss://silver@aprdemoadls.dfs.core.windows.net/temp_StockMovement\"\n",
    "\n",
    "# Write the DataFrame to a temporary path in CSV format\n",
    "df.coalesce(1).write.mode(\"overwrite\").csv(temp_output_path_csv, header=True)\n",
    "\n",
    "# List the files in the temporary directory\n",
    "files = dbutils.fs.ls(temp_output_path_csv)\n",
    "\n",
    "# Find the part file\n",
    "part_file = [file.path for file in files if file.path.endswith(\".csv\")][0]\n",
    "\n",
    "# Rename the part file to the desired output path\n",
    "dbutils.fs.mv(part_file, output_path_csv, True)\n",
    "\n",
    "# Remove the temporary directory\n",
    "dbutils.fs.rm(temp_output_path_csv, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43f9d08f-e014-4833-b207-bf8f3f7be67c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Read the CSV file to verify its contents\n",
    "df_csv = spark.read.csv(output_path_csv, header=True, inferSchema=True)\n",
    "display(df_csv)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Stock_Movement",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
