{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd1a206-e90d-401f-a9b0-bcb9168bb9ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.secrets.listScopes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8fdd2d6-3e35-42ed-8767-b6bca4426977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install databricks-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdefd0ff-4a8a-44b4-b61b-526265b13622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"aprdemoadls\"\n",
    "scope_name = \"keyvaultscopeap\"\n",
    "client_id = dbutils.secrets.get(scope=scope_name, key=\"clientid\")\n",
    "tenant_id = dbutils.secrets.get(scope=scope_name, key=\"tenantid\")\n",
    "client_secret = dbutils.secrets.get(scope=scope_name, key=\"secret\")\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f0d7ad6-f495-470a-a31e-1454c33bb75d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container_name = \"bronze\"\n",
    "file_name = \"Inventory 2\"\n",
    "storage_account_name = \"aprdemoadls\"\n",
    "file_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{file_name}.csv\"\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a21cb3d5-0d47-43a8-af46-765c8e5efbcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, when, col\n",
    "\n",
    "missing_values_df = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "display(missing_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7592e6a-3d2a-478d-94a9-db210fb2ede7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mode\n",
    "\n",
    "mode_value = df.select(mode(col(\"ReorderLevel\"))).collect()[0][0]\n",
    "df = df.na.fill({\"ReorderLevel\": mode_value})\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f87c2e-42d8-458c-a4a7-abc21e682510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, when, col\n",
    "\n",
    "missing_values_df = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "display(missing_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "804b07bb-c3f4-4291-8227-24b7f5af1ccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91c3a81c-0836-4452-a9b4-6cec0b83a1b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df.write.mode(\"overwrite\").parquet(\"abfss://silver@aprdemoadls.dfs.core.windows.net/inventory_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a4d6cd-a9bc-4e93-9764-751f8db68122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df.write.mode(\"overwrite\").csv(\"abfss://silver@aprdemoadls.dfs.core.windows.net/inventory_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34be3d43-20d8-4384-9fee-64baaf1e2670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Save the cleaned DataFrame to a single Parquet file\n",
    "output_path2 = \"abfss://silver@aprdemoadls.dfs.core.windows.net/Inventory.parquet\"\n",
    "temp_output_path = \"abfss://silver@aprdemoadls.dfs.core.windows.net/temp_Inventory.parquet\"\n",
    "\n",
    "# Write the DataFrame to a temporary path\n",
    "df.coalesce(1).write.mode(\"overwrite\").parquet(temp_output_path)\n",
    "\n",
    "# List the files in the temporary directory\n",
    "files = dbutils.fs.ls(temp_output_path)\n",
    "\n",
    "# Find the part file\n",
    "part_file = [file.path for file in files if file.path.endswith(\".parquet\")][0]\n",
    "\n",
    "# Rename the part file to the desired output path\n",
    "dbutils.fs.mv(part_file, output_path2 + \"/Inventory.parquet\", True)\n",
    "\n",
    "# Remove the temporary directory\n",
    "dbutils.fs.rm(temp_output_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a587b816-c9b6-4fb3-b0b9-de7e999cac75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Read the Parquet file to verify its contents\n",
    "df_parquet = spark.read.parquet(output_path2 + \"/Inventory.parquet\")\n",
    "display(df_parquet)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Inventory Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
